# LexCode Hybrid Stack ğŸš€

Ù‡Ù†Ø¯Ø³Ø© Ù‡Ø¬ÙŠÙ†Ø© Ù…ØªÙŠÙ†Ø©:
- **Rust (core/):** Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (HTTP/axum).
- **Node.js + TypeScript (services/api/):** Ø¨ÙˆØ§Ø¨Ø© APIØŒ Ù…ØµØ§Ø¯Ù‚Ø©ØŒ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø²ÙˆÙ‘Ø¯Ø§Øª.
- **Python (adapters/python/lexhub/):** ÙˆØµÙ„Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (OpenAI/Anthropic/HF/Kaggle...).

## Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹
```bash
cp .env.example .env
docker compose up --build
```
- Rust Core Ø¹Ù„Ù‰ `http://localhost:8080`
- API Gateway Ø¹Ù„Ù‰ `http://localhost:3000`


## Ø§Ø³ØªØ®Ø¯Ø§Ù… /v1/ai/infer (OpenAI)
Ø¶Ø¹ Ù…ÙØªØ§Ø­Ùƒ ÙÙŠ `.env`:
```
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ
OPENAI_BASE_URL=https://api.openai.com/v1  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ
```
Ø§Ø®ØªØ¨Ø±:
```bash
curl -X POST http://localhost:3000/v1/ai/infer \  -H "Content-Type: application/json" \  -d '{ "messages": [ { "role": "user", "content": "Ø¹Ø±Ù‘Ù LexCode ÙÙŠ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©." } ] }'
```

## RAG Engine Usage
- Ingest text/PDF documents: `python services/rag_engine/rag.py ingest --path ./docs --type auto`
- Query with context retrieval: `python services/rag_engine/rag.py query "question here" --top-k 5`

## Web UI
- Launch Streamlit UI: `streamlit run services/web_ui/streamlit_app.py`
- Select provider (OpenAI, Groq, Azure, local Phi-3, or mock) from the sidebar and optionally upload PDFs (stored locally).

## Docker-only RAG Stack
- Start full stack: `make rag-up` (uses `docker-compose.rag.yml` for Qdrant, local Phi runner, RAG API placeholder, and Web UI).

## Fine-tuning Dataset Prep
- Validate JSONL training data: `python scripts/fine_tune.py validate data/fine_tune/sample_training.jsonl`
- Sample datasets live in `data/fine_tune/` and can be extended before calling provider-specific fine-tuning APIs.
