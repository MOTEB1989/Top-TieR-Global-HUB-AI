#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
telegram_chatgpt_mode.py

Ø¨ÙˆØª ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… Ù…ØªÙ‚Ø¯Ù… ÙŠØ¹Ù…Ù„ ÙƒÙ€ ChatGPT Ø¯Ø§Ø®Ù„ Ù…Ø³ØªÙˆØ¯Ø¹ Top-TieR-Global-HUB-AI
- /chat        : Ø¯Ø±Ø¯Ø´Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù…
- /repo        : ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ø§Ø±ÙŠØ± ULTRA + ARCHITECTURE
- /insights    : Ù…Ù„Ø®Øµ Ø°ÙƒÙŠ Ø¹Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
- /file        : ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø¯Ø¦ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø³Ù„Ø©
- /status      : Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
- /help        : Ù…Ø³Ø§Ø¹Ø¯Ø©
- /whoami      : Ù…Ø¹Ø±ÙØ© Telegram ID Ù„Ø¥Ø¶Ø§ÙØªÙ‡ ÙÙŠ Allowlist

ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø£Ø³Ø±Ø§Ø±:
- TELEGRAM_BOT_TOKEN
- TELEGRAM_ALLOWLIST
- OPENAI_API_KEY
- OPENAI_MODEL (Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ø§ÙØªØ±Ø§Ø¶ÙŠ gpt-4o-mini)
- GITHUB_REPO (Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø·)
- ULTRA_PREFLIGHT_PATH / FULL_SCAN_SCRIPT / LOG_FILE_PATH (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ø¯Ù…Ø¬ Ø£Ø¹Ù…Ù‚)
"""

import os
import json
import logging
import textwrap
import subprocess
import math
from pathlib import Path
from typing import Dict, List, Any, Tuple

import requests
from telegram import Update, Document
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    ContextTypes,
    filters,
)

# Load .env file
from dotenv import load_dotenv
load_dotenv()

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„ ----------------------
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger("telegram_chatgpt_mode")

# ---------------------- Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© ----------------------
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
ALLOWLIST_ENV = os.getenv("TELEGRAM_ALLOWLIST", "").strip()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

GITHUB_REPO = os.getenv("GITHUB_REPO", "MOTEB1989/Top-TieR-Global-HUB-AI")

ULTRA_PREFLIGHT_PATH = os.getenv("ULTRA_PREFLIGHT_PATH", "scripts/ultra_preflight.sh")
FULL_SCAN_SCRIPT = os.getenv("FULL_SCAN_SCRIPT", "scripts/execute_full_scan.sh")
LOG_FILE_PATH = os.getenv("LOG_FILE_PATH", "analysis/ULTRA_REPORT.md")

CHAT_HISTORY_PATH = Path(os.getenv("CHAT_HISTORY_PATH", "analysis/chat_sessions.json"))
CHAT_HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)

# RAG Configuration
ENABLE_RAG = os.getenv("ENABLE_RAG", "false").lower() in ("true", "1", "yes")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
EMBEDDING_INDEX_PATH = os.getenv("EMBEDDING_INDEX_PATH", "analysis/embeddings/index.json")
VECTOR_TOP_K = int(os.getenv("VECTOR_TOP_K", "6"))

# ---------------------- Allowlist ----------------------
def parse_allowlist(raw: str):
    if not raw:
        return set()
    parts = [p.strip() for p in raw.split(",") if p.strip()]
    ids = set()
    for p in parts:
        try:
            ids.add(int(p))
        except ValueError:
            continue
    return ids

USER_ALLOWLIST = parse_allowlist(ALLOWLIST_ENV)


def is_authorized(update: Update) -> bool:
    """ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¶Ù…Ù† Ø§Ù„Ù€ Allowlist (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ØºÙŠØ± ÙØ§Ø±ØºØ©)."""
    if not USER_ALLOWLIST:
        # Ø¥Ø°Ø§ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ©: Ù†Ø³Ù…Ø­ Ù„Ù„Ø¬Ù…ÙŠØ¹ (ÙˆÙŠÙ…ÙƒÙ† Ù„Ø§Ø­Ù‚Ø§Ù‹ ØªØ´Ø¯ÙŠØ¯Ù‡Ø§)
        return True
    uid = update.effective_user.id if update.effective_user else None
    return uid in USER_ALLOWLIST


async def reject_if_unauthorized(update: Update) -> bool:
    if is_authorized(update):
        return False
    await update.message.reply_text(
        "âŒ ØºÙŠØ± Ù…ØµØ±Ø­ Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±.\n"
        "Ø§Ø³ØªØ®Ø¯Ù… /whoami Ø«Ù… Ø§Ø·Ù„Ø¨ Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø±ÙÙƒ Ø¥Ù„Ù‰ TELEGRAM_ALLOWLIST."
    )
    return True


# ---------------------- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø§Ù„ØªØ§Ø±ÙŠØ®) ----------------------
def load_sessions() -> Dict[str, List[Dict[str, str]]]:
    if not CHAT_HISTORY_PATH.exists():
        return {}
    try:
        with CHAT_HISTORY_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning("ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„ØªØ§Ø±ÙŠØ®: %s", e)
        return {}


def save_sessions(sessions: Dict[str, List[Dict[str, str]]]) -> None:
    try:
        with CHAT_HISTORY_PATH.open("w", encoding="utf-8") as f:
            json.dump(sessions, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error("ÙØ´Ù„ Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªØ§Ø±ÙŠØ®: %s", e)


def get_user_key(update: Update) -> str:
    uid = update.effective_user.id if update.effective_user else 0
    uname = update.effective_user.username or ""
    return f"{uid}:{uname}"


def append_message(
    sessions: Dict[str, List[Dict[str, str]]],
    user_key: str,
    role: str,
    content: str,
    max_messages: int = 30,
) -> None:
    if user_key not in sessions:
        sessions[user_key] = []
    sessions[user_key].append({"role": role, "content": content})
    # Ù‚ØµÙ‘ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¶Ø®Ù…
    if len(sessions[user_key]) > max_messages:
        sessions[user_key] = sessions[user_key][-max_messages:]


# ---------------------- Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ OpenAI ----------------------
class OpenAIError(Exception):
    pass


def call_openai_chat(
    messages: List[Dict[str, str]],
    model: str = None,
    temperature: float = 0.4,
    max_tokens: int = 700,
) -> str:
    if not OPENAI_API_KEY:
        raise OpenAIError("OPENAI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©")

    model = model or OPENAI_MODEL
    url = f"{OPENAI_BASE_URL.rstrip('/')}/chat/completions"

    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }

    resp = requests.post(url, json=payload, headers=headers, timeout=60)
    if resp.status_code != 200:
        logger.error("Ø®Ø·Ø£ Ù…Ù† OpenAI: %s - %s", resp.status_code, resp.text[:500])
        raise OpenAIError(f"OpenAI error {resp.status_code}: {resp.text[:200]}")

    data = resp.json()
    try:
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        logger.error("Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† OpenAI: %s | %s", e, data)
        raise OpenAIError("Unexpected OpenAI response structure")


def make_system_prompt() -> str:
    return textwrap.dedent(
        f"""
        Ø£Ù†Øª ÙˆÙƒÙŠÙ„ Ø°ÙƒÙŠ ÙŠØ¹Ù…Ù„ Ø¯Ø§Ø®Ù„ Ù…Ø³ØªÙˆØ¯Ø¹ GitHub Ø¨Ø§Ø³Ù… {GITHUB_REPO}.
        Ø¯ÙˆØ±Ùƒ:
        - Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø«Ù„ ChatGPT Ù„ÙƒÙ† Ù…Ø¹ ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:
          â€¢ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
          â€¢ Ø§Ù„Ø£Ù…Ù† ÙˆØ§Ù„Ø­ÙÙˆÙ’ÙƒÙ…Ø©
          â€¢ Ø§Ù„Ø£ØªÙ…ØªØ© (Agents / Workflows)
          â€¢ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„Ù€ CI/CD
        - Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨ Ù…Ø­ØªØ±ÙØŒ Ù…Ø®ØªØµØ±ØŒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ù…Ø§ Ù„Ù… ÙŠÙØ·Ù„Ø¨ ØºÙŠØ± Ø°Ù„Ùƒ.
        - Ø¹Ù†Ø¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ØŒ Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØµÙ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù† ØªÙˆÙØ±:
          â€¢ ARCHITECTURE.md
          â€¢ SECURITY_POSTURE.md
          â€¢ AGENT_PLAYBOOK.md
          â€¢ WORKFLOW_MAP.md
          â€¢ ULTRA_REPORT.md
        - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©: Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØªØ®Ù…ÙŠÙ†.
        """
    ).strip()


# ---------------------- RAG Embedding Functions ----------------------
def create_embedding(text: str) -> List[float]:
    """
    Create embedding for a single text using OpenAI API.
    Returns embedding vector or empty list on failure.
    """
    if not OPENAI_API_KEY:
        logger.error("Cannot create embedding: OPENAI_API_KEY not set")
        return []
    
    url = f"{OPENAI_BASE_URL.rstrip('/')}/embeddings"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    
    payload = {
        "model": EMBEDDING_MODEL,
        "input": [text],
    }
    
    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        return data["data"][0]["embedding"]
    
    except Exception as e:
        logger.error(f"Failed to create embedding: {e}")
        return []


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Calculate cosine similarity between two vectors.
    Handles edge cases: empty vectors, mismatched lengths.
    """
    if not vec1 or not vec2:
        return 0.0
    
    # Handle mismatched lengths by using shorter length
    min_len = min(len(vec1), len(vec2))
    vec1 = vec1[:min_len]
    vec2 = vec2[:min_len]
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(b * b for b in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return dot_product / (magnitude1 * magnitude2)


def load_embeddings_index() -> List[Dict[str, Any]]:
    """Load embeddings index from file. Returns empty list on failure."""
    index_path = Path(EMBEDDING_INDEX_PATH)
    if not index_path.is_absolute():
        repo_root = Path(__file__).parent.parent
        index_path = repo_root / index_path
    
    if not index_path.exists():
        logger.warning(f"Embeddings index not found: {index_path}")
        return []
    
    try:
        with index_path.open("r", encoding="utf-8") as f:
            index = json.load(f)
        logger.info(f"Loaded embeddings index with {len(index)} chunks")
        return index
    except Exception as e:
        logger.error(f"Failed to load embeddings index: {e}")
        return []


def retrieve_context_via_embeddings(query: str) -> Tuple[str, List[Dict[str, Any]]]:
    """
    Retrieve relevant context chunks from embeddings index.
    Returns (formatted_context, results_list).
    """
    if not ENABLE_RAG:
        return "", []
    
    index = load_embeddings_index()
    if not index:
        logger.warning("RAG enabled but index is empty or failed to load")
        return "", []
    
    # Create query embedding
    query_embedding = create_embedding(query)
    if not query_embedding:
        logger.error("Failed to create query embedding")
        return "", []
    
    # Calculate similarities
    results = []
    for chunk in index:
        if "embedding" not in chunk:
            continue
        
        similarity = cosine_similarity(query_embedding, chunk["embedding"])
        results.append((chunk, similarity))
    
    # Sort by similarity (highest first) and take top-k
    results.sort(key=lambda x: x[1], reverse=True)
    top_results = results[:VECTOR_TOP_K]
    
    if not top_results:
        return "", []
    
    # Format context
    context_parts = []
    result_list = []
    
    for i, (chunk, score) in enumerate(top_results):
        path = chunk.get("path", "unknown")
        content = chunk.get("content", "")
        
        context_parts.append(f"[{i+1}] {path} (score: {score:.3f})")
        context_parts.append(content[:500])  # Limit chunk size
        context_parts.append("")
        
        result_list.append({
            "rank": i + 1,
            "path": path,
            "score": round(score, 4),
            "content_preview": content[:200] + "..." if len(content) > 200 else content,
        })
    
    formatted_context = "\n".join(context_parts)
    logger.info(f"Retrieved {len(top_results)} context chunks for query")
    
    return formatted_context, result_list


# ---------------------- Ø£Ø¯ÙˆØ§Øª (Tools) ----------------------
def run_local_script(cmd: str) -> str:
    """ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Ø¨Øª Ù…Ø­Ù„ÙŠ (Ù…Ø«Ù„ preflight Ø£Ùˆ scan) ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª."""
    try:
        out = subprocess.check_output(
            cmd,
            shell=True,
            stderr=subprocess.STDOUT,
            encoding="utf-8",
            timeout=300,
        )
        return out[:3500]
    except subprocess.CalledProcessError as e:
        return f"âŒ ÙØ´Ù„ ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ù…Ø±:\n{cmd}\n\nØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:\n{e.output[:2000]}"
    except Exception as e:
        return f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ù…Ø±:\n{cmd}\n{e}"


def read_small_file(path: str, max_chars: int = 4000) -> str:
    p = Path(path)
    if not p.exists():
        return f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {path}"
    try:
        txt = p.read_text(encoding="utf-8", errors="ignore")
        if len(txt) > max_chars:
            return txt[:max_chars] + "\n...\n[ØªÙ… Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰]"
        return txt
    except Exception as e:
        return f"âŒ ØªØ¹Ø°Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {path}: {e}"


def build_repo_context() -> str:
    """Ø¬Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©/Ø§Ù„Ø£Ù…Ù†/Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±."""
    parts = []
    candidates = [
        "ARCHITECTURE.md",
        "SECURITY_POSTURE.md",
        "AGENT_PLAYBOOK.md",
        "WORKFLOW_MAP.md",
        "analysis/ULTRA_REPORT.md",
        "README.md",
    ]
    for path in candidates:
        if Path(path).exists():
            parts.append(f"\n===== {path} =====\n")
            parts.append(read_small_file(path, max_chars=2500))
    if not parts:
        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ù‡Ù†Ø¯Ø³ÙŠØ©/Ø£Ù…Ù†ÙŠØ© ÙƒØ§ÙÙŠØ© Ù„ØªÙ…Ø«ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹."
    return "\n".join(parts)


# ---------------------- Ø£ÙˆØ§Ù…Ø± ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… ----------------------
HELP_TEXT = textwrap.dedent(
    """
    ğŸ¤– *ÙˆØ¶Ø¹ ChatGPT Ù…ØªÙ‚Ø¯Ù… â€“ Top-TieR-Global-HUB-AI*

    Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©:

    /start      â†’ Ø±Ø³Ø§Ù„Ø© ØªØ±Ø­ÙŠØ¨
    /help       â†’ Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
    /whoami     â†’ Ø¹Ø±Ø¶ Telegram ID Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ù€ Allowlist

    ğŸ’¬ ÙˆØ¶Ø¹ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©:
    /chat Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„...
      â€¢ Ø¯Ø±Ø¯Ø´Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù…
      â€¢ Ù…Ø«Ø§Ù„: `/chat Ù…Ø§ Ù‡ÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ØŸ`

    ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ (RAG):
    /search Ù†Øµ Ø§Ù„Ø¨Ø­Ø«...
      â€¢ Ø¨Ø­Ø« Ù…ØªØ¬Ù‡ ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… embeddings
      â€¢ Ù…Ø«Ø§Ù„: `/search authentication implementation`
      â€¢ ÙŠØªØ·Ù„Ø¨ ØªÙØ¹ÙŠÙ„ ENABLE_RAG

    ğŸ§  ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©:
    /repo
      â€¢ ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ ARCHITECTURE/SECURITY/ULTRA_REPORT
    /insights
      â€¢ Ù…Ù„Ø®Øµ Ø°ÙƒÙŠ Ø¹Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ (Ù…Ø®Ø§Ø·Ø±ØŒ ÙØ±Øµ ØªØ­Ø³ÙŠÙ†ØŒ Ø£ÙˆÙ„ÙˆÙŠØ§Øª)

    ğŸ“‚ ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª:
    Ø£Ø±Ø³Ù„ Ù…Ù„ÙØ§Ù‹ Ù†ØµÙŠØ§Ù‹ (txt/md/json/log) Ø£Ùˆ Ø³ÙƒØ±Ø¨ØªØŒ ÙˆØ³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨ÙˆØª Ø¨ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø¯Ø¦ÙŠ Ù„Ù‡.

    âš™ï¸ Ø­Ø§Ù„Ø© ØªØ´ØºÙŠÙ„:
    /status
      â€¢ Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ† (OpenAI, GitHub, Allowlist, RAG)

    âš ï¸ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
    - Ø¨Ø¹Ø¶ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ù…ØªØ§Ø­Ø© ÙÙ‚Ø· Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¯Ø§Ø®Ù„ Allowlist (TELEGRAM_ALLOWLIST).
    """
).strip()


async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "ğŸ¤– Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ ÙˆØ¶Ø¹ ChatGPT Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¯Ø§Ø®Ù„ Ù…Ø³ØªÙˆØ¯Ø¹ Top-TieR-Global-HUB-AI.\n"
        "Ø§Ø³ØªØ®Ø¯Ù… /help Ù„Ø§Ø³ØªØ¹Ø±Ø§Ø¶ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©."
    )


async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_markdown(HELP_TEXT)


async def cmd_whoami(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    uid = update.effective_user.id if update.effective_user else "N/A"
    uname = update.effective_user.username if update.effective_user else ""
    await update.message.reply_text(
        f"ğŸ†” Ù…Ø¹Ø±ÙÙƒ ÙÙŠ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: `{uid}`\n"
        f"ğŸ‘¤ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: @{uname}\n\n"
        "Ø£Ø¶Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¹Ø±Ù ÙÙŠ TELEGRAM_ALLOWLIST (ÙƒÙ…Ø«Ø§Ù„):\n"
        f"TELEGRAM_ALLOWLIST={uid}"
    )


async def cmd_status(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    parts = []

    parts.append("ğŸ“Š *Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… â€“ ChatGPT Mode*")
    parts.append(f"- Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: `{GITHUB_REPO}`")

    # OpenAI
    if OPENAI_API_KEY:
        parts.append("ğŸ§  OpenAI: âœ… Ù…Ø¶Ø¨ÙˆØ· (OPENAI_API_KEY Ù…ÙˆØ¬ÙˆØ¯)")
        parts.append(f"   â€¢ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: `{OPENAI_MODEL}`")
    else:
        parts.append("ğŸ§  OpenAI: âŒ Ù…ÙÙ‚ÙˆØ¯ (OPENAI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯)")

    # RAG Status
    if ENABLE_RAG:
        parts.append("ğŸ” RAG: âœ… Ù…ÙØ¹Ù‘Ù„")
        parts.append(f"   â€¢ Ù†Ù…ÙˆØ°Ø¬ Embedding: `{EMBEDDING_MODEL}`")
        parts.append(f"   â€¢ Top-K: `{VECTOR_TOP_K}`")
        
        # Check index file
        index_path = Path(EMBEDDING_INDEX_PATH)
        if not index_path.is_absolute():
            repo_root = Path(__file__).parent.parent
            index_path = repo_root / index_path
        
        if index_path.exists():
            try:
                with index_path.open("r") as f:
                    index = json.load(f)
                parts.append(f"   â€¢ Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³: âœ… ({len(index)} chunks)")
            except Exception:
                parts.append(f"   â€¢ Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³: âš ï¸ Ù…ÙˆØ¬ÙˆØ¯ Ù„ÙƒÙ† Ø¨Ù‡ Ø®Ø·Ø£")
        else:
            parts.append(f"   â€¢ Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³: âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
    else:
        parts.append("ğŸ” RAG: âš ï¸ ØºÙŠØ± Ù…ÙØ¹Ù‘Ù„ (ENABLE_RAG=false)")

    # Allowlist
    if USER_ALLOWLIST:
        parts.append("ğŸ” Allowlist: âœ… Ù…ÙØ¹Ù‘Ù„")
        parts.append("   â€¢ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙˆÙ† Ø§Ù„Ù…ØµØ±Ø­ Ù„Ù‡Ù…:")
        for uid in USER_ALLOWLIST:
            parts.append(f"     - {uid}")
    else:
        parts.append("ğŸ” Allowlist: âš ï¸ ØºÙŠØ± Ù…ÙØ¹Ù‘Ù„ (Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ©ØŒ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ø³Ù…ÙˆØ­ Ù„Ù‡Ù… Ø­Ø§Ù„ÙŠØ§Ù‹)")

    # Ù…Ù„ÙØ§Øª Ù‡Ù†Ø¯Ø³ÙŠØ©
    exist_flags = []
    for p in ["ARCHITECTURE.md", "SECURITY_POSTURE.md", "AGENT_PLAYBOOK.md", "WORKFLOW_MAP.md", "analysis/ULTRA_REPORT.md"]:
        if Path(p).exists():
            exist_flags.append(f"   â€¢ âœ… {p}")
        else:
            exist_flags.append(f"   â€¢ âŒ {p}")
    parts.append("ğŸ“‚ Ù…Ù„ÙØ§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©/Ø§Ù„Ø£Ù…Ù†:")
    parts.extend(exist_flags)

    await update.message.reply_markdown("\n".join(parts))


async def cmd_chat(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    if await reject_if_unauthorized(update):
        return

    if not OPENAI_API_KEY:
        await update.message.reply_text("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… /chat Ù„Ø£Ù† OPENAI_API_KEY ØºÙŠØ± Ù…Ù‡ÙŠØ£.")
        return

    if not context.args:
        await update.message.reply_text(
            "âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø±.\n"
            "Ù…Ø«Ø§Ù„:\n"
            "/chat Ù…Ø§ Ù‡ÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ù€ CI/CD ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ØŸ"
        )
        return

    user_question = " ".join(context.args).strip()
    user_key = get_user_key(update)

    # Retrieve context via RAG if enabled
    rag_context = ""
    if ENABLE_RAG:
        retrieved_context, _ = retrieve_context_via_embeddings(user_question)
        if retrieved_context:
            rag_context = f"\n\n**Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ (RAG):**\n{retrieved_context}\n"
            logger.info(f"RAG context added to chat for user {user_key}")

    sessions = load_sessions()
    
    # Add user message with optional RAG context
    user_message_with_context = user_question
    if rag_context:
        user_message_with_context = user_question + rag_context
    
    append_message(sessions, user_key, "user", user_message_with_context)

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù…Ø¹ System Prompt + ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    messages = [{"role": "system", "content": make_system_prompt()}]
    messages.extend(sessions[user_key])

    try:
        reply = call_openai_chat(messages)
    except OpenAIError as e:
        await update.message.reply_text(f"âŒ Ø®Ø·Ø£ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:\n{e}")
        return

    append_message(sessions, user_key, "assistant", reply)
    save_sessions(sessions)

    # ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø±Ø¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹
    if len(reply) > 3500:
        reply = reply[:3500] + "\n...\n[ØªÙ… Ù‚Ø·Ø¹ Ø§Ù„Ø±Ø¯ Ù„Ø·ÙˆÙ„Ù‡]"

    await update.message.reply_text(reply)


async def cmd_repo(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    if await reject_if_unauthorized(update):
        return

    context_text = build_repo_context()

    if not OPENAI_API_KEY:
        # Ø¥Ø°Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ OpenAIØŒ Ù†Ø¹ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù…
        await update.message.reply_text(
            "âš ï¸ OPENAI_API_KEY ØºÙŠØ± Ù…Ù‡ÙŠØ£ØŒ Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©:\n\n"
            + context_text[:3500]
        )
        return

    prompt = textwrap.dedent(
        """
        Ø­Ù„Ù‘Ù„ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆÙ‚Ø¯Ù…:
        - Ù…Ù„Ø®Øµ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ (High-level summary)
        - Ø£Ø¨Ø±Ø² Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© Ø£Ùˆ Ø§Ù„Ø£Ù…Ù†ÙŠØ©
        - Ø£Ù‡Ù… Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©
        - 3 ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù‚ØµÙŠØ±Ø©

        Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹:
        """
    ).strip()

    messages = [
        {"role": "system", "content": make_system_prompt()},
        {"role": "user", "content": prompt + "\n\n" + context_text},
    ]

    try:
        reply = call_openai_chat(messages, max_tokens=700)
    except OpenAIError as e:
        await update.message.reply_text(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹:\n{e}")
        return

    await update.message.reply_text(reply[:3500])


async def cmd_insights(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    if await reject_if_unauthorized(update):
        return

    context_text = build_repo_context()

    if not OPENAI_API_KEY:
        await update.message.reply_text(
            "âš ï¸ OPENAI_API_KEY ØºÙŠØ± Ù…Ù‡ÙŠØ£ØŒ Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø®Ø§Ù… ÙÙ‚Ø·:\n\n"
            + context_text[:3500]
        )
        return

    prompt = textwrap.dedent(
        """
        ØªØµØ±Ù ÙƒÙ…Ù‡Ù†Ø¯Ø³ Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ù…Ù†ØµØ© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
        Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ØŒ Ø£Ø¹Ø·Ù†ÙŠ:

        1) Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Current State)
        2) Ø£Ù‡Ù… 5 Ù…Ø®Ø§Ø·Ø± Ø£Ùˆ ÙØ¬ÙˆØ§Øª (Risks / Gaps)
        3) Ø®Ø·Ø© ØªÙ†ÙÙŠØ° Ù…Ù† 3 Ù…Ø±Ø§Ø­Ù„ (Ù‚ØµÙŠØ±Ø©ØŒ Ù…ØªÙˆØ³Ø·Ø©ØŒ Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ø£Ø¬Ù„)
        4) Ø£ÙŠ ØªØ­Ø°ÙŠØ± ÙŠØ¬Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ù‡

        Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù‚Ø³Ù‘Ù…Ø© Ø¨Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ø¶Ø­Ø© ÙˆÙ†Ù‚Ø§Ø·.
        """
    ).strip()

    messages = [
        {"role": "system", "content": make_system_prompt()},
        {"role": "user", "content": prompt + "\n\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹:\n" + context_text},
    ]

    try:
        reply = call_openai_chat(messages, max_tokens=900)
    except OpenAIError as e:
        await update.message.reply_text(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù€ Insights:\n{e}")
        return

    await update.message.reply_text(reply[:3500])


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØªØ­Ù„ÙŠÙ„Ù‡ Ù…Ø¨Ø¯Ø¦ÙŠØ§Ù‹."""
    if await reject_if_unauthorized(update):
        return

    message = update.message
    doc: Document = message.document
    if not doc:
        return

    # ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªØ§Ù‹
    try:
        file = await doc.get_file()
        tmp_path = Path("analysis/uploads")
        tmp_path.mkdir(parents=True, exist_ok=True)
        local_file = tmp_path / f"{doc.file_unique_id}_{doc.file_name}"
        await file.download_to_drive(str(local_file))
    except Exception as e:
        await message.reply_text(f"âŒ ØªØ¹Ø°Ø± ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
        return

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ Ø¥Ø°Ø§ Ø£Ù…ÙƒÙ†
    suffix = local_file.suffix.lower()
    text_content = ""
    if suffix in [".txt", ".md", ".log", ".json", ".yaml", ".yml", ".py", ".ts", ".sh"]:
        try:
            text_content = local_file.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            await message.reply_text(f"âš ï¸ ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ù„ÙƒÙ† ØªØ¹Ø°Ø± Ù‚Ø±Ø§Ø¡ØªÙ‡ ÙƒÙ†Øµ: {e}")
            return
    else:
        await message.reply_text(
            f"ğŸ“ ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {local_file.name}\n"
            f"Ø§Ù„Ù†ÙˆØ¹: {suffix or 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'}\n\n"
            "Ø­Ø§Ù„ÙŠØ§Ù‹ Ù„Ø§ Ø£Ø¯Ø¹Ù… Ø¥Ù„Ø§ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø© (txt, md, log, json, yaml, py, ts, sh)."
        )
        return

    # Ø¥Ø°Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ OpenAI: Ù†Ø¹ÙŠØ¯ Ù…Ù‚ØªØ·Ù ÙÙ‚Ø·
    if not OPENAI_API_KEY:
        snippet = text_content[:1500]
        await message.reply_text(
            "âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEYØŒ Ø³Ø£Ø¹Ø±Ø¶ Ù…Ù‚ØªØ·ÙØ§Ù‹ Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù:\n\n" + snippet
        )
        return

    prompt = textwrap.dedent(
        f"""
        ØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù…Ø­ØªÙˆÙ‰ Ù…Ù„Ù Ù…Ù† Ù…Ø³ØªÙˆØ¯Ø¹ Ø¨Ø±Ù…Ø¬ÙŠ.

        Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
        - Ø£Ø¹Ø·Ù†ÙŠ Ù…Ù„Ø®ØµØ§Ù‹ Ù‚ØµÙŠØ±Ø§Ù‹ Ø¹Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù
        - Ø¥Ù† ÙƒØ§Ù† Ø³ÙƒØ±Ø¨Øª Ø£Ùˆ ÙƒÙˆØ¯: ÙˆØ¶Ù‘Ø­ Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠÙØ¹Ù„Ù‡
        - Ø¥Ù† ÙƒØ§Ù† ØªÙƒÙˆÙŠÙ† (config): ÙˆØ¶Ù‘Ø­ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø£Ùˆ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        - Ù„Ø§ ØªØ®Ù…Ù† Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù†Øµ ÙˆØ§Ø¶Ø­Ø§Ù‹ØŒ ÙˆÙ‚Ù„ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©" Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©

        Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù (Ù…Ù‚ØªØ·Ù):
        """
    ).strip()

    snippet = text_content[:4000]
    messages = [
        {"role": "system", "content": make_system_prompt()},
        {"role": "user", "content": prompt + "\n\n" + snippet},
    ]

    try:
        reply = call_openai_chat(messages, max_tokens=700)
    except OpenAIError as e:
        await message.reply_text(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù:\n{e}")
        return

    await message.reply_text(reply[:3500])


async def cmd_search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ ÙÙŠ ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹."""
    if await reject_if_unauthorized(update):
        return

    if not ENABLE_RAG:
        await update.message.reply_text(
            "âš ï¸ Ù…ÙŠØ²Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ ØºÙŠØ± Ù…ÙØ¹Ù‘Ù„Ø©.\n"
            "Ù„ØªÙØ¹ÙŠÙ„Ù‡Ø§ØŒ Ø§Ø¶Ø¨Ø· ENABLE_RAG=true ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©."
        )
        return

    if not OPENAI_API_KEY:
        await update.message.reply_text("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… /search Ù„Ø£Ù† OPENAI_API_KEY ØºÙŠØ± Ù…Ù‡ÙŠØ£.")
        return

    if not context.args:
        await update.message.reply_text(
            "âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ ÙƒØªØ§Ø¨Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø±.\n"
            "Ù…Ø«Ø§Ù„:\n"
            "/search authentication implementation"
        )
        return

    query = " ".join(context.args).strip()
    
    await update.message.reply_text("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹...")

    try:
        retrieved_context, results = retrieve_context_via_embeddings(query)
        
        if not results:
            await update.message.reply_text(
                "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬.\n"
                "ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ù…ÙˆØ¬ÙˆØ¯ ÙˆÙ„ÙŠØ³ ÙØ§Ø±ØºØ§Ù‹."
            )
            return
        
        # Format results for display
        response_parts = [f"ğŸ” *Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù„Ù€:* {query}\n"]
        
        for result in results:
            rank = result["rank"]
            score = result["score"]
            path = result["path"]
            preview = result["content_preview"]
            
            response_parts.append(f"**[{rank}] {path}** (score: {score})")
            response_parts.append(f"```\n{preview}\n```\n")
        
        response_text = "\n".join(response_parts)
        
        # Split if too long
        if len(response_text) > 3500:
            response_text = response_text[:3500] + "\n...\n[ØªÙ… Ù‚Ø·Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ø·ÙˆÙ„Ù‡Ø§]"
        
        await update.message.reply_markdown(response_text)
        logger.info(f"Search completed for query: {query}, found {len(results)} results")
    
    except Exception as e:
        logger.error(f"Search failed: {e}")
        await update.message.reply_text(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø«:\n{e}")


async def fallback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø±Ø³Ø§Ø¦Ù„ Ù†ØµÙŠØ© Ù„Ø§ ØªØ¨Ø¯Ø£ Ø¨Ø£Ù…Ø± /."""
    text = update.message.text or ""
    if not text.strip():
        return

    # ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§ ÙƒØ±Ø³Ø§Ù„Ø© Ø¯Ø±Ø¯Ø´Ø© Ù‚ØµÙŠØ±Ø©
    if not OPENAI_API_KEY:
        await update.message.reply_text(
            "ğŸ“¨ Ø§Ø³ØªÙ„Ù…Øª Ø±Ø³Ø§Ù„ØªÙƒØŒ Ù„ÙƒÙ† Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY Ø­Ø§Ù„ÙŠØ§Ù‹.\n"
            "Ø§Ø³ØªØ®Ø¯Ù… /help Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©."
        )
        return

    user_key = get_user_key(update)
    sessions = load_sessions()
    append_message(sessions, user_key, "user", text.strip())

    messages = [{"role": "system", "content": make_system_prompt()}]
    messages.extend(sessions[user_key])

    try:
        reply = call_openai_chat(messages, max_tokens=500)
    except OpenAIError as e:
        await update.message.reply_text(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø³Ø§Ù„Ø©:\n{e}")
        return

    append_message(sessions, user_key, "assistant", reply)
    save_sessions(sessions)

    await update.message.reply_text(reply[:3500])


# ---------------------- main ----------------------
def main() -> None:
    if not TELEGRAM_TOKEN:
        raise RuntimeError("âŒ TELEGRAM_BOT_TOKEN ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©")

    logger.info("Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Telegram ChatGPT Mode Bot ...")
    logger.info("Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: %s", GITHUB_REPO)
    if USER_ALLOWLIST:
        logger.info("Allowlist Ù…ÙØ¹Ù‘Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: %s", USER_ALLOWLIST)
    else:
        logger.warning("Allowlist ÙØ§Ø±Øº - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ø³Ù…ÙˆØ­ Ù„Ù‡Ù… Ø­Ø§Ù„ÙŠØ§Ù‹.")

    app = Application.builder().token(TELEGRAM_TOKEN).build()

    # Ø£ÙˆØ§Ù…Ø±
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("help", cmd_help))
    app.add_handler(CommandHandler("whoami", cmd_whoami))
    app.add_handler(CommandHandler("status", cmd_status))
    app.add_handler(CommandHandler("chat", cmd_chat))
    app.add_handler(CommandHandler("search", cmd_search))
    app.add_handler(CommandHandler("repo", cmd_repo))
    app.add_handler(CommandHandler("insights", cmd_insights))

    # Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…Ù„ÙØ§Øª
    app.add_handler(MessageHandler(filters.Document.ALL, handle_document))

    # fallback Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
    app.add_handler(
        MessageHandler(filters.TEXT & ~filters.COMMAND, fallback_handler)
    )

    app.run_polling()


if __name__ == "__main__":
    main()
